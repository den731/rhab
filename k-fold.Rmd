##  K-fold Cross-validation



Here we will verify each model by K-fold cross-validation. First, the full dataset will be split into two. Roughly 66% of the full set is for training and the remaining 33% for testing. 

On our training set, it is further divided into "K" folds. One fold is held away for validation, while the remaining are used to train the model. Next the trained model is validated against the validation fold. Each validation, a skill score is recorded. Here the RMSE is assessed along with its standard deviation. We can compare the RMSE between our models. The lowest RMSE is the best model. 

Ultimately, the trained model will further be tested again against the 33%. If the prediction is signifigant with the actual value of the training dataset it is deemed successful model.



Using the `library(caret)` package. 


Here each model that was found to be effective from previous section is tested. If the trained model fitted against the untrained dataset has a signifigant slope, then the model does have potential predictablility power, proving to be robust.

A 10 fold cross-validation with 20 repeated cycles will be done on a training dataset. K=10

### Successful Models

#### SUM ~ MCYE + OP 



```{r, echo=FALSE}



set.seed(123) 
zdata <- LOGTransformed %>% # Only select variables of interest
  select(SUM,   Developed, OP) %>%
  na.omit() # Omit all NA's

Dataframe <- as.data.frame(zdata) # Convert it as a dataframe

# Split our Data if we wanted to test it later. However not neccesary. This seems to be the old way. 

ind = createDataPartition(Dataframe$SUM, p=2/3, list=FALSE) 
traindata <- Dataframe[ind,] # Training dataset is 66%
testdata <- Dataframe[-ind,] # Testing dataset is 33%

#I was playing around with random forest. Ignore this.
#parameterGrid <- expand.grid(mtry=c(1,2,3)) 


# This is a stored setting variable 
# which sets up how the training resamples the data. 
train_control <- trainControl(method="repeatedcv", 
                              number =10, # The K fold
                              repeats=20  # Repeated times
                              
                              # , savePredictions = TRUE
                              )

model <- train(SUM~.,
                data=Dataframe,
               method="lm", # glm, rf, but no lme or mixed model support
               trControl = train_control
               #tuneGrid = parameterGrid
               )
```



Model Validation Performance:

```{r}


#mean(model$resample$Rsquared)
```


Our model: `r format(formula(model))` 


Across all 10-fold cross-validation, the average of the root mean square deviation:

(RMSE) = `r round(mean(model$resample$RMSE),2)` with $\sigma$ = `r round(sd(model$resample$RMSE),2)`. 

Average $R^2$ = `r round(mean(model$resample$Rsquared),2) ` with $\sigma$ = `r round(sd(model$resample$Rsquared),2)` 




```{r}
#sum.mod <- summary(model)
#sum.mod 
```

```{r}
#kable(sum.mod$coefficients, caption = "The averaged trained")
```


 
\newpage 

Evaluate the trained model and further validate this model on 33% of rest of the data. 

```{r, echo=FALSE}
Prediction <- predict(model,testdata, interval=c("confidence"))

plot(Prediction)
plot(Prediction~testdata$SUM, xlab="Predicted Values", ylab="Actual from Test Data")
abline(lm(Prediction~testdata$SUM))

```


```{r}
summary <- lm(testdata$SUM~Prediction)
out <- tidy(summary)
kable(out, digits = 3)
```
